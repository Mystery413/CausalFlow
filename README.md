# CausalFlow
Prior research on causal reasoning in large language models (LLMs) have primarily centered on proprietary models with high computational costs, leaving the capabilities of smaller, open-source models underexplored, particularly in low-resource environments. 
To bridge this gap, we introduce CausalFlow, a unified causal reasoning framework that unravels the causal inference capabilities of LLMs through two complementary tasks: (i) causal dependency prediction, where models answer binary causal queries prompted by structured statistical inputs; and (ii) causal graph discovery, which integrates LLMs with statistical skeleton learning methods to improve edge orientation in causal structures.
To enable lightweight adaptation, CausalFlow employs LoRA-based fine-tuning on open-source models such as DeepSeek-7B and LLaMA3â€“8B, achieving substantial gains in dependency prediction task on larger datasets within 24 GPU hours. For causal graph discovery, our hybrid approach combines PC-based skeleton recovery with zero-shot LLM prompting for edge directionality, yielding lower Structural Hamming Distance than standalone baselines, particularly on sparse or structurally simple Bayesian networks.
Extensive empirical results offer practical guidance for deploying CausalFlow in resource-constrained environments and support controlled, interpretable use of LLMs in causal reasoning pipelines. Moreover, its structured prompting and modular, plug-and-play design enable flexible adaptation to diverse real-world causal discovery scenarios across domains.
